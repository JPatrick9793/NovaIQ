{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = keras.datasets.imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=5000,\n",
    "                                                      skip_top=10,\n",
    "                                                      maxlen=255,\n",
    "                                                      seed=123,\n",
    "                                                      start_char=1,\n",
    "                                                      oov_char=2,\n",
    "                                                      index_from=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_idx = imdb.get_word_index()\n",
    "idx_2_word = {}\n",
    "\n",
    "for key, value in word_2_idx.items():\n",
    "    idx_2_word[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'the'), (2, 'and'), (3, 'a'), (4, 'of'), (5, 'to'), (6, 'is'), (7, 'br'), (8, 'in'), (9, 'it'), (10, 'i')]\n"
     ]
    }
   ],
   "source": [
    "print (sorted(idx_2_word.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=255)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for later:\n",
    ">model = tf.estimator.DNNClassifier(hidden_units=[128, 64, 32],\n",
    ">                                   feature_columns=___,\n",
    ">                                   model_dir='/DNNClassifier',\n",
    ">                                   n_classes=2,\n",
    ">                                   optimizer='Adam',\n",
    ">                                   activation_fn=tf.nn.relu,\n",
    ">                                   dropout=0.3,\n",
    ">                                   batch_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(\n",
    "    keras.layers.Embedding(input_dim=5000,\n",
    "                           output_dim=12,\n",
    "                           embeddings_initializer=keras.initializers.TruncatedNormal(\n",
    "                               mean=0.0, stddev=0.5, seed=123)))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(\n",
    "    keras.layers.Dense(units=64,\n",
    "                       activation=tf.nn.relu))\n",
    "model.add(\n",
    "    keras.layers.Dense(units=32,\n",
    "                       activation=tf.nn.relu))\n",
    "model.add(\n",
    "    keras.layers.Dense(units=1,\n",
    "                       activation=tf.nn.sigmoid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 12)          60000     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                832       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 62,945\n",
      "Trainable params: 62,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_cp = keras.callbacks.TensorBoard(log_dir='./logs',\n",
    "                                    histogram_freq=5,\n",
    "                                    write_graph=True,\n",
    "                                    write_grads=True,\n",
    "                                    batch_size=32,\n",
    "                                    write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    x_train, y_train,\n",
    "    test_size=0.25, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n",
      "[   2   14  641    2   18    2  119   65    2 3595    2   34  107  389\n",
      "  354   34    2    2    2    2    2    2   16  115   53  307   17    2\n",
      "    2    2 1696    2  255   37  892   41 1014   11    2    2    2    2\n",
      "    2 1708    2  731    2   23   27 1303   10   10   50    2    2   87\n",
      "  696  239   34    2    2   17    2    2   35 1732   17  210 3753    2\n",
      "  198  290    2 1866    2    2   93  200    2    2    2  255  467    2\n",
      " 2398   14    2   91    2   22   15  144 1229    2  483    2  259  262\n",
      "    2    2    2   37 2308   27  108   26   99  196    2    2 1185   33\n",
      "  211   93   61  350  747   15  291    2   24  110   34  195   84   35\n",
      "  709   46    2  158    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train[211]))\n",
    "print(X_train[211])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18750, 255)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18750 samples, validate on 6250 samples\n",
      "Epoch 1/30\n",
      "18750/18750 [==============================] - 4s 195us/step - loss: 0.6135 - acc: 0.6385 - val_loss: 0.4046 - val_acc: 0.8288\n",
      "Epoch 2/30\n",
      "18750/18750 [==============================] - 2s 130us/step - loss: 0.3307 - acc: 0.8556 - val_loss: 0.3405 - val_acc: 0.8493\n",
      "Epoch 3/30\n",
      "18750/18750 [==============================] - 2s 123us/step - loss: 0.2676 - acc: 0.8866 - val_loss: 0.2996 - val_acc: 0.8786\n",
      "Epoch 4/30\n",
      "18750/18750 [==============================] - 2s 125us/step - loss: 0.2340 - acc: 0.9049 - val_loss: 0.3019 - val_acc: 0.8778\n",
      "Epoch 5/30\n",
      "18750/18750 [==============================] - 3s 146us/step - loss: 0.2111 - acc: 0.9160 - val_loss: 0.3271 - val_acc: 0.8661\n",
      "Epoch 6/30\n",
      "18750/18750 [==============================] - 3s 137us/step - loss: 0.1949 - acc: 0.9241 - val_loss: 0.3279 - val_acc: 0.8706\n",
      "Epoch 7/30\n",
      "18750/18750 [==============================] - 2s 121us/step - loss: 0.1794 - acc: 0.9308 - val_loss: 0.3081 - val_acc: 0.8813\n",
      "Epoch 8/30\n",
      "18750/18750 [==============================] - 2s 123us/step - loss: 0.1696 - acc: 0.9358 - val_loss: 0.3212 - val_acc: 0.8781\n",
      "Epoch 9/30\n",
      "18750/18750 [==============================] - 2s 132us/step - loss: 0.1619 - acc: 0.9382 - val_loss: 0.3443 - val_acc: 0.8725\n",
      "Epoch 10/30\n",
      "18750/18750 [==============================] - 3s 161us/step - loss: 0.1533 - acc: 0.9431 - val_loss: 0.3362 - val_acc: 0.8773\n",
      "Epoch 11/30\n",
      "18750/18750 [==============================] - 2s 133us/step - loss: 0.1466 - acc: 0.9457 - val_loss: 0.3485 - val_acc: 0.8726\n",
      "Epoch 12/30\n",
      "18750/18750 [==============================] - 2s 124us/step - loss: 0.1432 - acc: 0.9464 - val_loss: 0.3534 - val_acc: 0.8723\n",
      "Epoch 13/30\n",
      "18750/18750 [==============================] - 2s 126us/step - loss: 0.1330 - acc: 0.9525 - val_loss: 0.3760 - val_acc: 0.8720\n",
      "Epoch 14/30\n",
      "18750/18750 [==============================] - 2s 131us/step - loss: 0.1262 - acc: 0.9556 - val_loss: 0.3843 - val_acc: 0.8680\n",
      "Epoch 15/30\n",
      "18750/18750 [==============================] - 3s 151us/step - loss: 0.1237 - acc: 0.9545 - val_loss: 0.4010 - val_acc: 0.8672\n",
      "Epoch 16/30\n",
      "18750/18750 [==============================] - 3s 140us/step - loss: 0.1156 - acc: 0.9607 - val_loss: 0.4108 - val_acc: 0.8667\n",
      "Epoch 17/30\n",
      "18750/18750 [==============================] - 3s 139us/step - loss: 0.1108 - acc: 0.9626 - val_loss: 0.4362 - val_acc: 0.8658\n",
      "Epoch 18/30\n",
      "18750/18750 [==============================] - 3s 138us/step - loss: 0.1052 - acc: 0.9662 - val_loss: 0.4512 - val_acc: 0.8653\n",
      "Epoch 19/30\n",
      "18750/18750 [==============================] - 3s 171us/step - loss: 0.1039 - acc: 0.9653 - val_loss: 0.4668 - val_acc: 0.8637\n",
      "Epoch 20/30\n",
      "18750/18750 [==============================] - 3s 160us/step - loss: 0.0956 - acc: 0.9692 - val_loss: 0.4951 - val_acc: 0.8613\n",
      "Epoch 21/30\n",
      "18750/18750 [==============================] - 2s 133us/step - loss: 0.0923 - acc: 0.9709 - val_loss: 0.4976 - val_acc: 0.8611\n",
      "Epoch 22/30\n",
      "18750/18750 [==============================] - 2s 128us/step - loss: 0.0892 - acc: 0.9723 - val_loss: 0.5431 - val_acc: 0.8605\n",
      "Epoch 23/30\n",
      "18750/18750 [==============================] - 3s 151us/step - loss: 0.0858 - acc: 0.9734 - val_loss: 0.5294 - val_acc: 0.8603\n",
      "Epoch 24/30\n",
      "18750/18750 [==============================] - 3s 142us/step - loss: 0.0859 - acc: 0.9733 - val_loss: 0.5475 - val_acc: 0.8581\n",
      "Epoch 25/30\n",
      "18750/18750 [==============================] - 2s 131us/step - loss: 0.0787 - acc: 0.9753 - val_loss: 0.6054 - val_acc: 0.8570\n",
      "Epoch 26/30\n",
      "18750/18750 [==============================] - 2s 131us/step - loss: 0.0775 - acc: 0.9764 - val_loss: 0.6238 - val_acc: 0.8530\n",
      "Epoch 27/30\n",
      "18750/18750 [==============================] - 3s 136us/step - loss: 0.0718 - acc: 0.9797 - val_loss: 0.6658 - val_acc: 0.8531\n",
      "Epoch 28/30\n",
      "18750/18750 [==============================] - 3s 146us/step - loss: 0.0711 - acc: 0.9789 - val_loss: 0.7146 - val_acc: 0.8509\n",
      "Epoch 29/30\n",
      "18750/18750 [==============================] - 3s 136us/step - loss: 0.0668 - acc: 0.9817 - val_loss: 0.7499 - val_acc: 0.8494\n",
      "Epoch 30/30\n",
      "18750/18750 [==============================] - 3s 144us/step - loss: 0.0632 - acc: 0.9831 - val_loss: 0.7247 - val_acc: 0.8544\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,\n",
    "                    Y_train,\n",
    "                    epochs=30,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_val, Y_val),\n",
    "                    verbose=1,\n",
    "                    callbacks=[tb_cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
